{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40eb95d5-78e4-4362-b80d-4b71331fe6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2966d444-55ac-419f-bd62-c0b5d21117de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013(data_dir):\n",
    "    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    emotion_to_label = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "    X, y = [], []\n",
    "    for subset in ['train', 'test']:\n",
    "        for emotion in emotions:\n",
    "            folder = os.path.join(data_dir, subset, emotion)\n",
    "            for img_file in os.listdir(folder):\n",
    "                if img_file.endswith('.jpg'):\n",
    "                    img = cv2.imread(os.path.join(folder, img_file), cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (48, 48))\n",
    "                        X.append(img)\n",
    "                        y.append(emotion_to_label[emotion])\n",
    "    X = np.array(X).reshape(-1, 48, 48, 1) / 255.0\n",
    "    y = np.array(y)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45d362e5-1b53-4a50-b16a-ce94f4045200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "\n",
    "def get_image_emotion(image, model):\n",
    "    img = Image.fromarray(image.astype('uint8')).convert('L').resize((48, 48))\n",
    "    img = np.expand_dims(np.array(img), axis=(0, -1)) / 255.0\n",
    "    return np.argmax(model.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "951901ae-3b98-4b37-863d-2b91598c1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_emotion(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "def get_audio_emotion(audio_path, model):\n",
    "    mfcc = extract_mfcc(audio_path).reshape(1, 1, -1)\n",
    "    return np.argmax(model.predict(mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e5f6b6d-44e9-4171-9d9b-9cac91c74dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_emotion(image, text, audio_path, image_model, text_model, tokenizer, audio_model):\n",
    "    preds = [\n",
    "        get_image_emotion(image, image_model),\n",
    "        get_text_emotion(text, text_model, tokenizer),\n",
    "        get_audio_emotion(audio_path, audio_model)\n",
    "    ]\n",
    "    return max(set(preds), key=preds.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81b9d6b1-50cc-4ba7-907f-4d3fd7f2b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_to_music = {\n",
    "#     0: {\"mood\": \"calm\", \"valence\": 0.2},\n",
    "#     1: {\"mood\": \"joy\", \"valence\": 0.8},\n",
    "#     2: {\"mood\": \"angry\", \"valence\": 0.1},\n",
    "#     3: {\"mood\": \"fear\", \"valence\": 0.1},\n",
    "#     4: {\"mood\": \"surprise\", \"valence\": 0.6},\n",
    "#     5: {\"mood\": \"neutral\", \"valence\": 0.5},\n",
    "#     6: {\"mood\": \"disgust\", \"valence\": 0.3}\n",
    "# }\n",
    "\n",
    "emotion_to_music = {\n",
    "    0: {\"mood\": \"chill\", \"valence\": 0.2},\n",
    "    1: {\"mood\": \"pop\", \"valence\": 0.8},\n",
    "    2: {\"mood\": \"rock\", \"valence\": 0.1},          # was metal ‚Üí now valid\n",
    "    3: {\"mood\": \"ambient\", \"valence\": 0.1},\n",
    "    4: {\"mood\": \"electronic\", \"valence\": 0.6},\n",
    "    5: {\"mood\": \"acoustic\", \"valence\": 0.5},\n",
    "    6: {\"mood\": \"blues\", \"valence\": 0.3}\n",
    "}\n",
    "# sp = spotipy.Spotify(auth_manager=spotipy.SpotifyOAuth(\n",
    "#     client_id=\"YOUR_SPOTIFY_CLIENT_ID\",\n",
    "#     client_secret=\"YOUR_SPOTIFY_CLIENT_SECRET\",\n",
    "#     redirect_uri=\"http://localhost:8888/callback\",\n",
    "#     scope=\"user-read-private user-read-email\"\n",
    "# ))\n",
    "\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    client_id=\"8a0d569b187a42a0a88de5fd710b0f36\", client_secret=\"53f9fab414e846ab95bd9347de5a83c8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "930fb6fb-f0c9-400b-a10c-d7a8765a3893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_music(emotion_label):\n",
    "    mood = emotion_to_music[emotion_label]['mood']\n",
    "    valence = emotion_to_music[emotion_label]['valence']\n",
    "    try:\n",
    "        results = sp.recommendations(seed_genres=[mood], target_valence=valence, limit=5)\n",
    "        return [f\"{t['name']} by {t['artists'][0]['name']}: {t['external_urls']['spotify']}\" for t in results['tracks']]\n",
    "    except Exception as e:\n",
    "        return [f\"Error fetching recommendations: {e}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "985d9c3b-6b28-4a75-9aed-b0ba81a3a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def emotion_music_app(image, text, audio_file):\n",
    "#     emotion = fused_emotion(image, text, audio_file, model_fer, model_text, tokenizer, model_audio)\n",
    "#     mood = emotion_to_music[emotion]['mood']\n",
    "#     recommendations = recommend_music(emotion)\n",
    "#     return f\"Detected Emotion: {mood.capitalize()}\\n\\nTop Songs:\\n\" + \"\\n\".join(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67919fdb-03d2-48cd-ae8f-1b978d39d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_fer = load_model(\"fer_model.h5\")\n",
    "model_audio = load_model(\"best_audio_lstm_model.h5\")\n",
    "model_text = BertForSequenceClassification.from_pretrained(\"./saved_model/bert_goemotions\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./saved_model/bert_goemotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0de6dc9a-547e-4eb7-9b1c-d3d7415de407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 22:37:58.509 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\mmish\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-07-05 22:37:58.512 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "st.set_page_config(page_title=\"Emotion-Based Music Recommender\", layout=\"centered\")\n",
    "st.markdown(\"<h1 style='text-align: center; color: #4A90E2;'>üéµ Emotion-Aware Music Recommendation System</h1>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"Upload your **facial image**, enter your **text input**, and **upload a voice clip** to get personalized music recommendations based on your emotion!\")\n",
    "\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "with col1:\n",
    "    image_file = st.file_uploader(\"üì∑ Facial Image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "with col2:\n",
    "    text_input = st.text_input(\"üí¨ Text Input\", placeholder=\"How are you feeling today?\")\n",
    "\n",
    "with col3:\n",
    "    audio_file = st.file_uploader(\"üé§ Audio File (WAV)\", type=[\"wav\"])\n",
    "\n",
    "if st.button(\"üéØ Detect Emotion & Recommend Music\"):\n",
    "    if image_file and text_input and audio_file:\n",
    "        with st.spinner(\"üîç Detecting your emotion...\"):\n",
    "            try:\n",
    "                image = Image.open(image_file).convert('L')\n",
    "                image_np = np.array(image)\n",
    "                emotion = fused_emotion(image_np, text_input, audio_file.name, model_fer, model_text, tokenizer, model_audio)\n",
    "                mood = emotion_to_music[emotion]['mood']\n",
    "                valence = emotion_to_music[emotion]['valence']\n",
    "\n",
    "                st.success(f\"üß† Detected Emotion: **{mood.capitalize()}** (Valence: {valence})\")\n",
    "\n",
    "                st.markdown(\"---\")\n",
    "                st.subheader(\"üéß Recommended Tracks\")\n",
    "\n",
    "                recommendations = recommend_music(emotion)\n",
    "                for i, track in enumerate(recommendations, 1):\n",
    "                    st.markdown(f\"{i}. {track}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                st.error(f\"‚ùå An error occurred during processing:\\n{e}\")\n",
    "    else:\n",
    "        st.warning(\"‚ö†Ô∏è Please provide **all three inputs**: image, text, and audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ca9e6-9c5f-470f-8a02-d2a8bee6b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec11ae1-4fe5-442a-9402-1d88aaf9609f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
