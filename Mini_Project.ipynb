{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40eb95d5-78e4-4362-b80d-4b71331fe6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import gradio as gr\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2966d444-55ac-419f-bd62-c0b5d21117de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013(data_dir):\n",
    "    emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    emotion_to_label = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "    X, y = [], []\n",
    "    for subset in ['train', 'test']:\n",
    "        for emotion in emotions:\n",
    "            folder = os.path.join(data_dir, subset, emotion)\n",
    "            for img_file in os.listdir(folder):\n",
    "                if img_file.endswith('.jpg'):\n",
    "                    img = cv2.imread(os.path.join(folder, img_file), cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (48, 48))\n",
    "                        X.append(img)\n",
    "                        y.append(emotion_to_label[emotion])\n",
    "    X = np.array(X).reshape(-1, 48, 48, 1) / 255.0\n",
    "    y = np.array(y)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d362e5-1b53-4a50-b16a-ce94f4045200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_goemotions(data_dir):\n",
    "    emotion_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "    df = pd.concat([pd.read_csv(f) for f in emotion_files], ignore_index=True)\n",
    "    emotion_cols = [col for col in df.columns if col not in ['text', 'id']]\n",
    "    X, y = df['text'], df[emotion_cols].idxmax(axis=1)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def extract_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951901ae-3b98-4b37-863d-2b91598c1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ravdess(data_dir):\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    X, y = [], []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                emotion_code = file.split('-')[2]\n",
    "                if emotion_code in emotion_map:\n",
    "                    X.append(extract_mfcc(os.path.join(root, file)))\n",
    "                    y.append(emotion_map[emotion_code])\n",
    "    X = np.array(X)\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    return *train_test_split(X, y, test_size=0.2, random_state=42), le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5f6b6d-44e9-4171-9d9b-9cac91c74dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fer_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f027dd-a1ce-443c-9d42-7dd43e0a4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229a54d1-17a6-4ba7-931f-e2b3e3b432f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_emotion(image, model):\n",
    "    img = Image.fromarray(image.astype('uint8')).convert('L').resize((48, 48))\n",
    "    img = np.expand_dims(np.array(img), axis=(0, -1)) / 255.0\n",
    "    return np.argmax(model.predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5aa04e-847a-47d6-bfca-8a6ee79b12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_emotion(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2564b98a-c934-444d-829c-e88d316b14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_emotion(audio_path, model):\n",
    "    mfcc = extract_mfcc(audio_path).reshape(1, 1, -1)\n",
    "    return np.argmax(model.predict(mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31a52953-9d9b-401d-b7c3-c3eac1655fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_emotion(image, text, audio_path, image_model, text_model, tokenizer, audio_model):\n",
    "    preds = [\n",
    "        get_image_emotion(image, image_model),\n",
    "        get_text_emotion(text, text_model, tokenizer),\n",
    "        get_audio_emotion(audio_path, audio_model)\n",
    "    ]\n",
    "    return max(set(preds), key=preds.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58afaf8e-67d0-4471-b2d0-53ff04fa12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Valid Spotify genres:\", sp.recommendation_genre_seeds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81b9d6b1-50cc-4ba7-907f-4d3fd7f2b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion_to_music = {\n",
    "#     0: {\"mood\": \"calm\", \"valence\": 0.2},\n",
    "#     1: {\"mood\": \"joy\", \"valence\": 0.8},\n",
    "#     2: {\"mood\": \"angry\", \"valence\": 0.1},\n",
    "#     3: {\"mood\": \"fear\", \"valence\": 0.1},\n",
    "#     4: {\"mood\": \"surprise\", \"valence\": 0.6},\n",
    "#     5: {\"mood\": \"neutral\", \"valence\": 0.5},\n",
    "#     6: {\"mood\": \"disgust\", \"valence\": 0.3}\n",
    "# }\n",
    "\n",
    "emotion_to_music = {\n",
    "    0: {\"mood\": \"chill\", \"valence\": 0.2},\n",
    "    1: {\"mood\": \"pop\", \"valence\": 0.8},\n",
    "    2: {\"mood\": \"rock\", \"valence\": 0.1},          # was metal → now valid\n",
    "    3: {\"mood\": \"ambient\", \"valence\": 0.1},\n",
    "    4: {\"mood\": \"electronic\", \"valence\": 0.6},\n",
    "    5: {\"mood\": \"acoustic\", \"valence\": 0.5},\n",
    "    6: {\"mood\": \"blues\", \"valence\": 0.3}\n",
    "}\n",
    "sp = spotipy.Spotify(auth_manager=spotipy.SpotifyOAuth(\n",
    "    client_id=\"YOUR_SPOTIFY_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_SPOTIFY_CLIENT_SECRET\",\n",
    "    redirect_uri=\"http://localhost:8888/callback\",\n",
    "    scope=\"user-read-private user-read-email\"\n",
    "))\n",
    "\n",
    "\n",
    "#sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    #client_id=\"8a0d569b187a42a0a88de5fd710b0f36\", client_secret=\"53f9fab414e846ab95bd9347de5a83c8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "930fb6fb-f0c9-400b-a10c-d7a8765a3893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_music(emotion_label):\n",
    "    mood = emotion_to_music[emotion_label]['mood']\n",
    "    valence = emotion_to_music[emotion_label]['valence']\n",
    "    results = sp.recommendations(seed_genres=[mood], target_valence=valence, limit=5)\n",
    "    return [f\"{t['name']} by {t['artists'][0]['name']}: {t['external_urls']['spotify']}\" for t in results['tracks']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "985d9c3b-6b28-4a75-9aed-b0ba81a3a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def emotion_music_app(image, text, audio_file):\n",
    "#     emotion = fused_emotion(image, text, audio_file, model_fer, model_text, tokenizer, model_audio)\n",
    "#     mood = emotion_to_music[emotion]['mood']\n",
    "#     recommendations = recommend_music(emotion)\n",
    "#     return f\"Detected Emotion: {mood.capitalize()}\\n\\nTop Songs:\\n\" + \"\\n\".join(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9548a94-ea1b-4d31-bba5-d6a826b3d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_music_app(image, text, audio_file):\n",
    "    try:\n",
    "        emotion = fused_emotion(image, text, audio_file, model_fer, model_text, tokenizer, model_audio)\n",
    "        mood = emotion_to_music[emotion]['mood']\n",
    "        recommendations = recommend_music(emotion)\n",
    "        return f\"Detected Emotion: {mood.capitalize()}\\n\\nTop Songs:\\n\" + \"\\n\".join(recommendations)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback_str = traceback.format_exc()\n",
    "        print(traceback_str)  # This will print in Jupyter output\n",
    "        return f\"❌ Error occurred:\\n{str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68f891a6-ee4b-4006-8115-58525c35bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui = gr.Interface(\n",
    "    fn=emotion_music_app,\n",
    "    inputs=[\n",
    "        gr.Image(label=\"Facial Image\"),\n",
    "        gr.Textbox(label=\"Text Input\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Audio Input (WAV)\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Multimodal Emotion-Based Music Recommender\",\n",
    "    description=\"Upload a face image, type text, and record voice to receive emotion-aware music recommendations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67919fdb-03d2-48cd-ae8f-1b978d39d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model_fer = load_model(\"fer_model.h5\")\n",
    "model_audio = load_model(\"best_audio_lstm_model.h5\")\n",
    "model_text = BertForSequenceClassification.from_pretrained(\"./saved_model/bert_goemotions\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./saved_model/bert_goemotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0de6dc9a-547e-4eb7-9b1c-d3d7415de407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gui.launch(server_name=\"127.0.0.1\", server_port=7862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ca9e6-9c5f-470f-8a02-d2a8bee6b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec11ae1-4fe5-442a-9402-1d88aaf9609f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
